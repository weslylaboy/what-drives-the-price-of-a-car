{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Task Definition\n",
    "\n",
    "The goal is to build a supervised regression model to find the relationship between the price of a used car and its features. The target variable is the price of the car, and the features include various attributes such as make, model, year, mileage, and condition. The goal is to identify which features are most influential in determining the price of a used car and to provide actionable insights for the used car dealership to optimize their inventory and pricing strategies.\n",
    "\n",
    "This task involves data preprocessing, feature engineering, and multiple model building using regression techniques. The models' coefficients will be compared to identify the most significant drivers of used car prices. The performance of the models will be evaluated using appropriate metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) to ensure that the predictions are accurate and reliable for business decision making.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Steps for Data Understanding\n",
    "1. Load the dataset and perform an initial inspection to understand its structure, the number of rows and columns, data types, and a preview the first and last records.\n",
    "2. Check for missing values and determine how to handle them, fill them with mean, mode, or delete them.\n",
    "3. Explore the distribution of the target variable (price) and identify any outliers.\n",
    "4. Identify any categorical variables and explore unique values.\n",
    "5. Identify any data quality issues, such as duplicates, inconsistency, or errors, and plan how to address them."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T21:42:43.550554Z",
     "start_time": "2026-02-21T21:42:42.998449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, RidgeCV\n",
    "\n",
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/vehicles.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation:*** The dataset contains 426,000 rows and 20 columns. This indicates that we have a large dataset to work with, which can provide valuable insights into the factors that influence used car prices."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head(20)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.tail()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***Interpretation:*** From describe function output I can see that the price, year, and odometer may need cleaning. The minimum price is `$0` and the maximum is `$3,736,928,711` wich may be an error in data. The minimum year is `1900` which may be an error. The minimum odometer value is `0` which is wrong because the data is about used cars, while the maximum is `$10,000,000` which may be wrong unless is a classic car.\n",
    "\n",
    "I will need to handle outliers by using the IQR method. I also need to set up a lower bound for the price and odometer columns since the minimum values are 0. For the year, I will set a lower bound of year `2000`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function that takes a dataframe a returns a new dataframe with the total of nulls and percentage of nulls for each column in dataframe\n",
    "def calculate_percentage_of_nulls_by_column(dataframe):\n",
    "    null_total = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "    null_percentage = (dataframe.isnull().sum() * 100 / dataframe.isnull().count()).round(2).sort_values(ascending=False)\n",
    "    null_df = pd.DataFrame({'Total Nulls': null_total, 'Percentage of Nulls (%)': null_percentage})\n",
    "    return null_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "calculate_percentage_of_nulls_by_column(df)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### What would be the size of the data if I drop all missing entries?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "non_null_df = df.dropna()\n",
    "non_null_df.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation:*** Cannot remove all missing values because the resulting dataframe will be too small."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for unique values in categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:*** Data has lots of missing values, so we need to handle them before proceeding with analysis."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print unique values for each column\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].unique()}\")  # Print first 10 unique values for each categorical column\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***Interpretation:*** Above, I calculated the total number of null values and their percentage for each column in the dataset. The resulting dataframe is sorted by the total number of nulls in descending order, the variable with the highest number of nulls will appear first.\n",
    "\n",
    "If I drop all missing entries, the resulting dataframe will have `34,868` rows, a significant reduction from the original `426,880` rows. First, I will consider removing the unnecessary columns, and then try again removing the missing entries to validate the amount of data we have left. Based on that result, I will decide which is a better approach if removing the missing entries or fill the data with the mode, mean, or any other value.\n",
    "\n",
    "The `size` column has the highest number of null values, missing `71.8%` of its entries, which indicates that it may not be a reliable feature for modeling. The `cylinders` column is missing `41.6%` of its entries, which is also a significant amount of missing data. The `condition` column is missing `40.79%` of entries, also a significant amount. The `drive` and `paint_color` columns are missing `30.5%` of entries. The `type` column is missing `21.8%` of entries. The other columns are missing less than `5%` of their entries, which may be more manageable.\n",
    "\n",
    "I will handle the missing data as follows:\n",
    "- I will drop the `size` column, as it has the highest number of null values, missing `71.8%` of its entries, which indicates that it may not be a reliable feature for modeling.\n",
    "- For `cylinders`, since is missing `41.6%` of values, if I fill the data with the mode (most common value), it could reduce the variance of the data and cause potential bias. This is a categorical variable, one of the possible values is `other`, I will assign `other` to all missing entries.\n",
    "- The `condition` column is also missing significant amount of entries `40.79%`.  This is also a categorical column, I will create a new categorical value called `unknown` and fill the missing values with it. The model will learn if the absence of this information has an impact on price.\n",
    "- For `drive`, it's missing a significant amount of entries `30.6%`. This is also a categorical column, I will create a new categorical value called `unknown` and fill the missing values with it. The model will learn if the absence of this information has an impact on price.\n",
    "- For `paint_color` column, the value varies by brand and year. I will create new categorical value called `unknown` and fill the missing values with it.\n",
    "- For `type`, I will also consider filling the missing values with value `other`, which is a value already present in the column.\n",
    "- For other columns with less than `5%` missing values, I will consider removing the rows with missing values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_price_distribution(data, column='price', bins=50, color='#636EFA'):\n",
    "    counts, bin_edges = np.histogram(data[column], bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    agg_df = pd.DataFrame({\"bin_center\": bin_centers, \"count\": counts})\n",
    "    fig = px.bar(agg_df, x=\"bin_center\", y=\"count\",\n",
    "                 labels={'bin_center': 'Price ($)', 'count': 'Number of Cars'},\n",
    "                 title='Distribution of Car Prices',\n",
    "                 color_discrete_sequence=[color])\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    fig.show()\n",
    "\n",
    "plot_price_distribution(df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation:*** The distribution is not clearly seen because there are some extreme outliers in the price variable. To better visualize the distribution, I will apply a logarithmic transformation to the price variable and then plot the histogram again."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_log_price_distribution(data, column='price', bins=50, color='#636EFA'):\n",
    "    prices = np.log1p(df[column]).to_numpy()\n",
    "\n",
    "    counts, bin_edges = np.histogram(prices, bins=50)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    agg_df = pd.DataFrame({\"price\": bin_centers, \"count\": counts})\n",
    "    fig = px.bar(agg_df,\n",
    "                 x=\"price\",\n",
    "                 y=\"count\",\n",
    "                 labels={'price': 'Log-Transformed Price', 'count': 'Number of Cars'},\n",
    "                 title='Distribution of Car Prices (Log Transformed)',\n",
    "                 color_discrete_sequence=['#EF553B']\n",
    "                 )\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    fig.show()\n",
    "\n",
    "plot_log_price_distribution(df)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation***: The price in this data has some extreme outliers. The log transformation helps to reduce the impact of these outliers and provides a more normal distribution of the price variable. However, I will remove the outliers using the IQR method."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean data\n",
    "# drop unusable columns: size (many missing values), VIN (unique id, not useful), id (inventory id, not useful)\n",
    "clean_df = df.drop(columns=['size', 'VIN', 'id'])\n",
    "clean_df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove rows with missing values in columns with less than 5% of nan\n",
    "clean_df.dropna(subset=['manufacturer', 'title_status', 'model', 'odometer', 'fuel', 'transmission'], inplace=True)\n",
    "clean_df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Describe function shows outliers or errors given that min values for price and odometer is `0` and the minimum year is `1900`. Still need to fix this data issues."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove Outliers"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Price outliers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# First, remove price less than 500, is not realistic having a car cost less than $500\n",
    "clean_df = clean_df[clean_df['price'] >= 500]\n",
    "# The ceiling will be the top 1%.\n",
    "upper_price_limit = clean_df['price'].quantile(0.99)\n",
    "# Remove the top 1%\n",
    "clean_df = clean_df[clean_df['price'] <= upper_price_limit]\n",
    "# Remove Price outliers using IQR method. For price I will set a lower bound of 500 to remove prices below $500\n",
    "# Q1 = clean_df['price'].quantile(0.25)\n",
    "# Q3 = clean_df['price'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# Minimum is $500\n",
    "# lower_price_limit = max(500, Q1 - 1.5 * IQR)\n",
    "# upper_price_limit = Q3 + 1.5 * IQR\n",
    "# lower_price_limit = clean_df['price'].quantile(0.01)\n",
    "# print(lower_price_limit)\n",
    "# Remove outliers\n",
    "# clean_df = clean_df[(clean_df['price'] >= lower_price_limit) & (clean_df['price'] <= upper_price_limit)]\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Price outliers were removed. The new minimum price is `$500` and the maximum is `$255,000`. Which is a reasonable range for used cars, but may need to come back to this step depending on model results."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Odometer Outliers\n",
    "\n",
    "Odometer has extreme outliers. The minimum being 0 and maximum over 10 million. I will set a lower bound of 500 and remove the top 1% odometer values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean odometer. Remove the odometer with value = 0\n",
    "# Q1 = clean_df['odometer'].quantile(0.25)\n",
    "# Q3 = clean_df['odometer'].quantile(0.75)\n",
    "# print(Q1)\n",
    "# print(Q3)\n",
    "# IQR = Q3 - Q1\n",
    "# print(IQR)\n",
    "# lower_price_limit = max(500, Q1 - 1.5 * IQR)\n",
    "# upper_price_limit = Q3 + 1.5 * IQR\n",
    "#\n",
    "# print(Q1 - 1.5 * IQR)\n",
    "# print(Q3+ 1.5 * IQR)\n",
    "# Remove outliers\n",
    "# clean_df = clean_df[(clean_df['odometer'] >= (Q1 - 1.5 * IQR)) & (clean_df['odometer'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Set lower bound, remove values less than 500\n",
    "clean_df = clean_df[clean_df['odometer'] >= 500]\n",
    "# The ceiling will be the top 1%.\n",
    "upper_odometer_limit = clean_df['odometer'].quantile(0.99)\n",
    "# Remove the top 1%\n",
    "clean_df = clean_df[clean_df['odometer'] <= upper_odometer_limit]\n",
    "\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Odometer outliers were removed based on above conditions. The minimum value is `500` while the maximum is `267,692`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Year Outliers\n",
    "\n",
    "I will remove all entries before the year `2000`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clean_df = clean_df[clean_df['year'] >= 2000]\n",
    "clean_df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Year outliers were removed based on above conditions. The minimum value is `2000` while the maximum is `2022`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Categorical Columns Cleaning"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "calculate_percentage_of_nulls_by_column(clean_df)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The above dataframe that I still have the categorical columns with missing values."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Cylinders Column Cleansing\n",
    "\n",
    "I will fill the missing values with `other`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The possible values for cylinders are the following: ['8 cylinders', '6 cylinders', '4 cylinders', '5 cylinders','other', '3 cylinders', '10 cylinders', '12 cylinders']. I will assign other to all missing entries.\n",
    "\n",
    "clean_df['cylinders'].fillna('other', inplace=True)\n",
    "clean_df['cylinders'].unique()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Missing values were filled with `other`. From above output, we can confirm the possible values are `['8 cylinders', '6 cylinders', '4 cylinders', '5 cylinders', 'other', '3 cylinders', '10 cylinders', '12 cylinders']`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Condition Column"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The `condition` column will be filled with `unknown`.\n",
    "\n",
    "clean_df['condition'].fillna('unknown', inplace=True)\n",
    "clean_df['condition'].unique()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Missing values were filled with `unknown`. From above output, we can confirm the possible values are `['excellent', 'fair', 'like new', 'new', 'salvage', 'unknown']`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Drive Column\n",
    "- For `drive`, I will fill all missing values with `unknown`.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Fill drive missing values with unknown\n",
    "clean_df['drive'].fillna('unknown', inplace=True)\n",
    "clean_df['drive'].unique()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:***** All missing values were filled with `unknown`. From above output, we can confirm the possible values are `['4WD', 'RWD', 'FWD', 'unknown']`. Our model will learn if missing this value is important for price prediction."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Paint Color Column\n",
    "\n",
    "- For `paint_color` column, I will create new categorical value called `unknown` and fill the missing values with it.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clean_df['paint_color'].fillna('unknown', inplace=True)\n",
    "clean_df['paint_color'].unique()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:***** All missing values were filled with `unknown`. From above output, we can confirm the possible values are `['white', 'blue', 'red', 'black', 'silver', 'grey', 'unknown', 'brown', 'yellow', 'orange', 'custom', 'green', 'purple']`. Our model will learn if missing this value is important for price prediction."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Type Column\n",
    "\n",
    "For `type`, I will use the value `other` to fill all missing values in this column.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# - For `type`,filling the missing values with value `other`\n",
    "clean_df['type'].fillna('other', inplace=True)\n",
    "clean_df['type'].unique()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:***** All missing values were filled with `other`. From above output, we can confirm the possible values are `['pickup', 'truck', 'other', 'coupe', 'SUV', 'hatchback',\n",
    "       'mini-van', 'sedan', 'offroad', 'convertible', 'wagon', 'van',\n",
    "       'bus']`. Our model will learn if missing this value is important for price prediction."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Validate for missing values"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Validate how many nan are left\n",
    "calculate_percentage_of_nulls_by_column(clean_df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.head()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For other columns with less than `5%` missing values, I will remove the rows with missing values.\n",
    "clean_df.dropna(inplace=True)\n",
    "calculate_percentage_of_nulls_by_column(clean_df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation:*** From above dataframe, I can confirm that all missing values have been fixed."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Validate duplicate rows"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count total exact duplicates\n",
    "print(f\"Exact duplicates: {clean_df.duplicated().sum()}\")\n",
    "\n",
    "# View the duplicate rows\n",
    "duplicate_rows = clean_df[clean_df.duplicated()]\n",
    "duplicate_rows.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** From above I can see that there are `40,155` duplicate rows. I need to remove the duplicates to avoid data redundancy.\n",
    "\n",
    "But what if I only consider duplicates based on core car features?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check duplicates based on core car features\n",
    "core_features = ['price', 'year', 'manufacturer', 'model', 'odometer', 'state']\n",
    "duplicates = clean_df.duplicated(subset=core_features).sum()\n",
    "print(f\"Logical duplicates based on car features: {duplicates}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** Using only the columns `['price', 'year', 'manufacturer', 'model', 'odometer', 'state']` as core features, I found more duplicates than original, the total duplicates is `99,616`. I will remove duplicates based on core features to avoid data redundancy."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clean_df = clean_df.drop_duplicates(subset=core_features, keep='first')\n",
    "duplicates = clean_df.duplicated(subset=core_features).sum()\n",
    "print(f\"Logical duplicates based on car features: {duplicates}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** After removing duplicates based on core features, I found `0` duplicates, which means all duplicates have been removed."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.head()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clean_df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** Dataset looks balanced and ready for modeling.\n",
    "\n",
    "- For price, we have a minimum value of `$500` and a maximum of `$66,500`.\n",
    "- The years range for our cleaned data is from 2000 to 2022.\n",
    "- The odometer readings range from `500` to `269,504` miles.\n",
    "    - Cars with low odometer readings are generally cars from year 2022\n",
    "    - Cars with high odometer readings are olders cars"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Plots\n",
    "\n",
    "Now that that is cleaned, I can explore the data using plots to better understand the distribution of car prices and other features."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clean_df.boxplot(column='price', patch_artist=True,\n",
    "                 boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                 medianprops=dict(color='red'),\n",
    "                 flierprops=dict(marker='o', markerfacecolor='orange', markersize=8, markeredgecolor='white'))\n",
    "plt.title('Boxplot of Car Prices')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.xlabel('Car Price')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** From the boxplot, we can see that the car prices are generally distributed between `$8,000` and `$27,000`, with a few higher price above `$53,000` which can be explained because are recent models. The median price is around `$10,000`.\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_distribution(dataframe, x_column, bins = 30, kde = True, title=\"\", x_label=\"\",y_label=''):\n",
    "    sns.histplot(data=dataframe, x=x_column, bins=bins, kde=kde)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution(clean_df, x_column='price',\n",
    "                  title='Distribution of Car Prices',\n",
    "                  x_label=\"Price ($)\",\n",
    "                  y_label=\"Frequency\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The distribution of car prices is right skewed with a long tail towards higher prices. I will use a logarithm transformation to normalize the price distribution."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Apply log transformation to price column"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clean_df['log_price'] = np.log1p(clean_df['price'])\n",
    "plot_distribution(clean_df, x_column='log_price',\n",
    "                  title='Distribution of Car Prices',\n",
    "                  x_label=\"Price ($)\",\n",
    "                  y_label=\"Frequency\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** After applying log1p transformation to the price column, the plot now looks left skew starting with a long tail towards lower prices. This is because there are a lot of cars in the range of `$500`. I will need to compare the log skew value between the cleaned price and original price. If the skew is between `-0.5` and `0.5`, the data is considered 'faily symmetrical' and we can continue without more price transformation."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Original Skew:\", clean_df['price'].skew()) # Assuming you kept a backup\n",
    "print(\"Log Skew:\", clean_df['log_price'].skew())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Interpretation:**** Although the `log_price` variable has a skew of `-0.66` (outside the ideal range of `[-0.5 ,0.5]`), it is still within the acceptable range of `[-1,1]` for modeling compared to the `0.96` skewness for the original price column. Since the logarithmic transformation helps stabilize variance and reduces the impact of extreme prices, I will proceed with this transformation but remain attentive to model errors in lower-priced cars `($500â€“$2,000)`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Relations between features\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Price vs Odometer Reading\n",
    "\n",
    "Visualize cost depreciation based on odometer reading\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot for linear reg\n",
    "sns.regplot(data=clean_df,\n",
    "            x='odometer', y='price',\n",
    "            scatter=False,\n",
    "            line_kws={'color': 'red',\n",
    "                      'linewidth': 2,\n",
    "                      'label': 'Overall Trend'\n",
    "                      }\n",
    "            )\n",
    "\n",
    "#  Plot for colored scatterplot by condition\n",
    "sns.scatterplot(data=clean_df, x='odometer', y='price',\n",
    "                hue='condition', alpha=0.3, s=30, palette='viridis')\n",
    "\n",
    "\n",
    "# Add legend outside the plot area to avoid overlap\n",
    "plt.legend(title='Condition', bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "           frameon=True, fancybox=True, shadow=True, fontsize=10)\n",
    "\n",
    "plt.title('Price ($) vs. Odometer Reading')\n",
    "plt.xlabel('Odometer (miles)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The scatter plot shows a negative correlation between odometer reading and price, with a few outliers. The regression line suggests that as the odometer reading increases, the price tends to decrease. However, the relationship is not perfectly linear, and there are some cars with high mileage that still have relatively high prices. This could be due to factors such as condition, age, or unique features of the car. The scatter plot also shows that there are many cars with low mileage that have high prices, which could indicate that these cars are in high demand or have unique features that justify their price."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Year vs Price\n",
    "\n",
    "Visualize how price changes over car age"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Calculate both mean and median\n",
    "stats_by_year = clean_df.groupby('year')['price'].agg(['mean', 'median']).reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot both mean\n",
    "plt.plot(stats_by_year['year'], stats_by_year['mean'],\n",
    "         marker='o', linewidth=2.5, label='Mean Price', color='steelblue')\n",
    "# Plot both median\n",
    "plt.plot(stats_by_year['year'], stats_by_year['median'],\n",
    "         marker='s', linewidth=2.5, label='Median Price',\n",
    "         color='coral', linestyle='--')\n",
    "\n",
    "plt.title('Car Price Statistics by Year', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Year', fontsize=15)\n",
    "plt.ylabel('Price ($)', fontsize=15)\n",
    "plt.legend(fontsize=14, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The plots shows that olders cars have lower average prices, indicating that car age is a significant factor in determining used car prices. There is a drop in price for cars after the year 2020, which may be due to changes in technology or consumer preferences or an error in data, I need to take a look at this later if I notice issues with the model. This suggests that car age is a significant factor in determining used car prices, and that newer cars may be more valuable than older cars."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Price vs Condition\n",
    "\n",
    "Visualize how price changes over car condition"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Set style and figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# boxplot for condition\n",
    "sns.boxplot(data=clean_df, x='condition', y='price',\n",
    "            palette='Set2', linewidth=2, fliersize=5)\n",
    "\n",
    "plt.title('Car Price Distribution by Condition', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Condition', fontsize=13)\n",
    "plt.ylabel('Price ($)', fontsize=13)\n",
    "\n",
    "# Rotate x-labels if needed\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** The plot for car price by condition shows that condition `fair` and `salvage` are the lowest in price, however, I can see outliers with high variability with price higher than `$30,000`, this could mean car that are luxury or an error in the data.\n",
    "\n",
    "Condition `new` and `like new` are show median prices between `$15,000` and `$18,000`. The IQR ranges are the biggest one, showing consistency in price between these two conditions.\n",
    "\n",
    "Condition `good` have higher median, close to `$19,000`, compared to `new`, `like new`, and `excellent`.\n",
    "\n",
    "Based on above observations, it seems that some categories have outliers within certain conditions, which could indicate potential errors or unique characteristics of those vehicles. I need to take this into consideration because this feature may not be reliable to predict prices.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Price vs Transmission\n",
    "\n",
    "Visualize how the price is affected by transmission type"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set style and figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# boxplot for condition\n",
    "sns.barplot(data=clean_df, x='drive', y='price',\n",
    "            palette='Set2',\n",
    "            errorbar='ci',  # Show confidence interval\n",
    "            capsize=0.1,    # Add caps to error bars\n",
    "            linewidth=2)\n",
    "\n",
    "plt.title('Average Car Price by Drive Type', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Drive Type', fontsize=15)\n",
    "plt.ylabel('Average Price ($)', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** Drive with `4WD` AND `RWD` tend to have higher average prices, compared to `FWD` cars are more expensive on average. So I can conclude that the correlation is strong with price indicatiand that `4WD` > `RWD` > `unknown` > `FWD`.\n",
    "\n",
    "The `unknown` value which is the new category created for missing data, could be helpful in understanding the impact of missing transmission data on car prices. This value could help me find additional pricing patterns."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Multivariate Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Heatmap\n",
    "\n",
    "Verify relationships between `price`, `year`, `odometer`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.heatmap(clean_df[['price', 'year', 'odometer']].corr(numeric_only=True),\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            fmt='.2f',          # Format to 2 decimal places\n",
    "            linewidths=0.5,     # Add gridlines\n",
    "            linecolor='white',  # White gridline color\n",
    "            )\n",
    "\n",
    "plt.title('Correlation Matrix of Car Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** This heatmap provides insights into the relationships between these key car features and their impact on `price`. The highest positive correlation is between `price` and `year` with `0.64`, which indicates that newer cars tend to have higher prices. Additionally, there is a moderate negative correlation between `price` and `odometer` with `-0.59`, suggesting that cars with lower mileage tend to be more expensive. There is also a strong negative correlation of `-0.70` between `year` and `odometer`, which confirms that older cars tends to have higher milage.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Top 15 manufacturers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Get top 15 manufacturer\n",
    "top_manufacturer = clean_df['manufacturer'].value_counts().head(15).index\n",
    "\n",
    "\n",
    "medians = clean_df[clean_df['manufacturer'].isin(top_manufacturer)].groupby('manufacturer')['price'].median()\n",
    "\n",
    "bars = (clean_df['manufacturer'].value_counts().head(15)\n",
    "    .plot(\n",
    "        kind='barh',\n",
    "        color='teal',\n",
    "        edgecolor='black'))\n",
    "\n",
    "plt.title('Top 15 Car Manufacturers (Median Price Labels)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Number of Vehicles', fontsize=14)\n",
    "plt.ylabel('Manufacturer', fontsize=18)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add median labels on bars\n",
    "ax = plt.gca()\n",
    "for i, patch in enumerate(ax.patches):\n",
    "    width = patch.get_width()\n",
    "    mfg = top_manufacturer[i]\n",
    "    median_val = int(medians[mfg]) if pd.notna(medians[mfg]) else 0\n",
    "\n",
    "    # Position inside bar near end, white text over teal\n",
    "    ax.text(width * 0.99, patch.get_y() + patch.get_height()/2,\n",
    "            f'Median: ${median_val:,}', ha='right', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Reduce right margin for more space\n",
    "plt.subplots_adjust(right=0.85)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** The above plot shows the top 15 manufacturer and also the median by manufacturer. The top 5 manufacturers are `ford`, `chevrolet`, `toyota`, `honda`, and `nissan` showing used car market dominance. This suggests that these brands are popular among consumers and may have a significant presence in the used car market. I can also note that even though `ram` and `gmc` are not in the top 5 manufacturers, they have the biggest median with `$29,990` and `$21,999` respectively, showing higher prices. Dealers may consider focusing on these brands for inventory and marketing strategies.\n",
    "\n",
    "Also, I can confirm the existing of some luxury brand like `bmw` and `mercedes-benz` as top 15 manufactorer. These brands may explain the existence of outliers in `price` column. These brands may also have a significant presence in the used car market, and dealers should consider their offerings  luxury brands as well."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Luxury Manufacturer Price\n",
    "\n",
    "Now I would like to explore luxury brand to confirm the existence of outliers in `price` column is due to these luxury brands."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define luxury brands and filter the dataframe\n",
    "luxury_brands = ['porsche', 'lexus', 'audi', 'cadillac', 'mercedes-benz', 'bmw', 'infiniti', 'lincoln', 'tesla', 'jaguar', 'land rover', 'alfa-romeo', 'aston-martin', 'ferrari', 'lamborghini']\n",
    "luxury_manufacturer_df = clean_df[clean_df['manufacturer'].isin(luxury_brands)]\n",
    "\n",
    "# Get the top 15 luxury manufacturers by median price for ordering\n",
    "top_15_luxury_manufacturers = luxury_manufacturer_df.groupby('manufacturer')['price'].median().sort_values(ascending=False).head(15).index\n",
    "\n",
    "# Create the boxplot with high-visibility outliers\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Box plot with outlier in yellow\n",
    "sns.boxplot(data=luxury_manufacturer_df,\n",
    "            x='manufacturer',\n",
    "            y='price',\n",
    "            order=top_15_luxury_manufacturers,\n",
    "            palette='viridis',\n",
    "            flierprops={\"marker\": \"o\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"yellow\", \"markersize\": 9},\n",
    "            medianprops={\"color\": \"white\", \"linewidth\": 4})\n",
    "\n",
    "\n",
    "plt.title('Top 15 Brands by Median Price', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Manufacturer', fontsize=16)\n",
    "plt.ylabel('Price ($)', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=17)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** The boxplot shows the distribution of prices for the top 15 luxury car manufacturers. The median price for each manufacturer is indicated by a white line, and the yellow dots represent outliers. The plot reveals that some luxury brands have significantly outliers, for example `aston-martin` have values near to 0, same happens with `tesla`, and `alfa-romeo`. For brands like `mercedes-benz`, `lincoln`, `jeep`, and `cadillac`, reveals outliers with prices up to `$63,000`.\n",
    "\n",
    "Surprising findings are, `ram` having a higher median than `porche`, possibly becuause `ram` are dominating the market.\n",
    "\n",
    "Depending on modeling results, I may need to investigate further this outliers and confirm if they are correct or not. I can probably increase the IQR range to capture more outliers, currently I'm using a lower bound of `$500` and applied a `99th percentile cap` (`quantile(0.99)`).\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Price vs State/Region\n",
    "\n",
    "The dataset includes geographic data `state` and `region` which can be used to explore regional pricing trends."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "state_median = clean_df.groupby('state')['price'].median().sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(20, 24))\n",
    "state_median.plot(kind='barh', color='teal', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "plt.title('Median Car Price by US State', fontsize=24, fontweight='bold')\n",
    "plt.xlabel('Median Price ($)', fontsize=22)\n",
    "plt.ylabel('State', fontsize=22)\n",
    "plt.xticks( fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The above charts shows how car prices vary accross different US states. This geography analysis is crucial for used car dealers accross the country as the could optimize their inventory based on user's car preference by state."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### State vs Car type\n",
    "\n",
    "I would like to explore the preference between cars type(`suv`, `sedan`,etc) by contry state."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate the percentage of each car type by state\n",
    "type_state = pd.crosstab(clean_df['state'], clean_df['type'])\n",
    "type_state_pct = type_state.div(type_state.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotting\n",
    "ax = type_state_pct.plot(kind='barh', stacked=True, figsize=(22, 40), colormap='tab20')\n",
    "\n",
    "# Increase Title size\n",
    "plt.title('Car Type Preference by State (%)', fontsize=26, fontweight='bold', pad=20)\n",
    "\n",
    "# Increase Axis Labels size\n",
    "plt.xlabel('Percentage (%)', fontsize=26, labelpad=15)\n",
    "plt.ylabel('State', fontsize=22, labelpad=15)\n",
    "\n",
    "# Increase Legend size and adjust position\n",
    "plt.legend(title='Car Type', title_fontsize=22, fontsize=24, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "# Increase Tick Labels size (State names and Percentages)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "# grid for easier reading of percentages\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The above chart shows the percentage of car types preferred by each state. This analysis is crucial for used car dealers as it helps them understand the car type demand in different regions and optimize their inventory accordingly. The `suv` is the largerst segment in almost every state. The category `other` which was introduced to fill missing values is tacking a large proportion in this plots. I may need to take a closer look at this category later."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "To create new features based on columns combinations, this will help the model to better capture car value. Before creating new features, I need to split the data into train and test to avoid data leakage. I will split the data into `80%` train and `20%` test."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Split data into train and test"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Features and target\n",
    "X = clean_df.drop(columns=['price', 'log_price'])\n",
    "y = clean_df['log_price']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### New age column\n",
    "\n",
    "First, I will create a new feature column that will reflect the age of the car based on the year. For this excersice, I will assume the current year is `2023`. This will create a new column with a integer value containing the age.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# assume the current year is 2023\n",
    "current_year = 2023\n",
    "\n",
    "# create a new column with age (current year - car year)\n",
    "X_train['age'] = current_year - X_train['year']\n",
    "X_test['age']  = current_year - X_test['year']\n",
    "X_train.describe()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** A new `age` column was created. Since we assumed the current year is `2023` and since the cut off year for our data is year `2000`, the minimum age is 1 and the maximum is 23."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Car model column transformation\n",
    "\n",
    "The `model` column has `29,649` unique values. Since it has lots of value, I will use frequency encoding. Frequency encoding replaces each value in the model column with the value of how many that specific model appears in the data. This option is better because it creates just one numeric feature instead of thousands of dummies.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:00:20.935775Z",
     "start_time": "2026-02-21T18:00:20.849702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate the frequency for model\n",
    "model_frequency = X_train['model'].value_counts() / len(X_train)\n",
    "\n",
    "# Map the value of the model to model_frequency column\n",
    "X_train['model_frequency'] = X_train['model'].map(model_frequency).fillna(0)\n",
    "X_test['model_frequency'] = X_test['model'].map(model_frequency).fillna(0)\n",
    "\n",
    "X_test.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               year       odometer           age  model_frequency\n",
       "count  44879.000000   44879.000000  44879.000000     44879.000000\n",
       "mean    2012.437777   96295.319994     10.562223         0.002123\n",
       "std        5.120087   59670.839921      5.120087         0.003322\n",
       "min     2000.000000     500.000000      1.000000         0.000000\n",
       "25%     2009.000000   43975.000000      6.000000         0.000123\n",
       "50%     2013.000000   93347.000000     10.000000         0.000613\n",
       "75%     2017.000000  139252.000000     14.000000         0.002908\n",
       "max     2022.000000  269362.000000     23.000000         0.018082"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>odometer</th>\n",
       "      <th>age</th>\n",
       "      <th>model_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44879.000000</td>\n",
       "      <td>44879.000000</td>\n",
       "      <td>44879.000000</td>\n",
       "      <td>44879.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2012.437777</td>\n",
       "      <td>96295.319994</td>\n",
       "      <td>10.562223</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.120087</td>\n",
       "      <td>59670.839921</td>\n",
       "      <td>5.120087</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>43975.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2013.000000</td>\n",
       "      <td>93347.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>139252.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.002908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>269362.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.018082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Region transformation\n",
    "\n",
    "Similar to the `model` column the `region` is a high cardinality column. I will use frequency encoding. Frequency encoding replaces each value in the region column with the value of how many that specific region appears in the data. This option is better because it creates just one numeric features instead of thousands of dummies.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:00:41.380594Z",
     "start_time": "2026-02-21T18:00:41.295159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate the frequency for region\n",
    "region_frequency = X_train['region'].value_counts() / len(X_train)\n",
    "# Map the value of the model to region_frequency column\n",
    "X_train['region_frequency'] = X_train['region'].map(region_frequency).fillna(0)\n",
    "X_test['region_frequency'] = X_test['region'].map(region_frequency).fillna(0)\n",
    "print(f'Missing values in test for region: {X_test['model_frequency'].isnull().sum()}')\n",
    "\n",
    "X_train.describe()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in test for region: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                year       odometer            age  model_frequency  \\\n",
       "count  179512.000000  179512.000000  179512.000000    179512.000000   \n",
       "mean     2012.436244   95928.169744      10.563756         0.002140   \n",
       "std         5.124851   59588.662865       5.124851         0.003345   \n",
       "min      2000.000000     500.000000       1.000000         0.000006   \n",
       "25%      2009.000000   43675.250000       6.000000         0.000128   \n",
       "50%      2013.000000   93000.000000      10.000000         0.000613   \n",
       "75%      2017.000000  138780.750000      14.000000         0.002986   \n",
       "max      2022.000000  269504.000000      23.000000         0.018082   \n",
       "\n",
       "       region_frequency  \n",
       "count     179512.000000  \n",
       "mean           0.005479  \n",
       "std            0.002912  \n",
       "min            0.000017  \n",
       "25%            0.002663  \n",
       "50%            0.006189  \n",
       "75%            0.008027  \n",
       "max            0.010295  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>odometer</th>\n",
       "      <th>age</th>\n",
       "      <th>model_frequency</th>\n",
       "      <th>region_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>179512.000000</td>\n",
       "      <td>179512.000000</td>\n",
       "      <td>179512.000000</td>\n",
       "      <td>179512.000000</td>\n",
       "      <td>179512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2012.436244</td>\n",
       "      <td>95928.169744</td>\n",
       "      <td>10.563756</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.005479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.124851</td>\n",
       "      <td>59588.662865</td>\n",
       "      <td>5.124851</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.002912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>43675.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.002663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2013.000000</td>\n",
       "      <td>93000.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.006189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>138780.750000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.008027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>269504.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>0.010295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The `region` column was transformed into a new numerical column called `region_frequency`. This column represents the frequency of each region in the dataset. This transformation helps in capturing the relative importance of each region without creating a large number of dummy variables."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pipeline for ordinal and categorical columns transformation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Columns categorization\n",
    "\n",
    "- For `condition`\n",
    "    - The `condition` column has `6` unique values with a specific order (`'good' 'excellent' 'fair' 'like new' 'new' 'salvage')`. I will apply an Ordinal Encoder from sckitlearn.  The encoder will start from 1 in case I choose later to use a multiplicate model, which `1` will ensure that the lowest category doesn't get canceled.\n",
    "- For columns `fuel`, `transmission`, `paint_color`, `drive`, `type`, and `title_status`\n",
    "    -The `fuel` (5 unique values), `transmission` (3 unique values), `paint_color` (12 unique values), `drive` (3 unique values), `type` (13 unique values), and `title_status` (6 unique values) columns are categorical variables with low to moderate cardinality. I will use One-Hot encoding for these columns. One-Hot encoding creates a binary column for each unique value in the categorical column, indicating the presence or absence of that value for each row.\n",
    "- For columns `odometer`, `age`, `model_frequency`, and `region_frequency`\n",
    "    - These columns are numericals, I will apply a standard scaler to these columns to normalize the data and improve the performance of the model."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:00:48.742829Z",
     "start_time": "2026-02-21T18:00:48.728906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ordinal_columns = ['condition']\n",
    "# order for condition column, from lowest to highest\n",
    "condition_order = [['salvage', 'fair', 'unknown', 'good', 'excellent', 'like new', 'new']]\n",
    "\n",
    "categorical_columns = ['fuel', 'transmission', 'paint_color', 'drive', 'type', 'title_status']\n",
    "numeric_columns = ['odometer', 'age', 'model_frequency', 'region_frequency']\n"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Pipeline creation\n",
    "\n",
    "I will create a column transformer to apply the appropriate transformation for each type of column. Then I will create a pipeline that includes the column transformer, linear regression model, and Lasso regression model. This pipeline will allow me to easily apply the same transformations and modeling steps to both the training and test data.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:00:54.440978Z",
     "start_time": "2026-02-21T18:00:54.412686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ordinal', OrdinalEncoder(categories=condition_order), ordinal_columns),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False), categorical_columns),\n",
    "        ('scaler', StandardScaler(), numeric_columns)\n",
    "    ],\n",
    "    remainder='drop'  # Drop other columns that are not specified\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "****Interpretation:**** The `condition` column was transformed into a new numerical column called `condition_encoded` using OrdinalEncoder. The `unknown` value was placed between `fair` and `good`. This assumes that most cars with a missing `condition` are likely in \"average\" or \"slightly below average\" `condition`.\n",
    "\n",
    "A hot encoding was applied to the columns `fuel`, `transmission`, `paint_color`, `drive`, `type`, and `title_status`. This transformation creates binary columns for each unique value in these categorical columns, allowing the model to capture the presence or absence of specific categories.\n",
    "\n",
    "A standard scaler was applied to the columns `odometer`, `age`, `model_frequency`, and `region_frequency`. This transformation normalizes the data, which can improve the performance of certain machine learning models by ensuring that all features are on a similar scale. The `remainder='passthrough'` parameter ensures that any columns not specified in the transformers are left unchanged in the output of the column transformer."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Find the best alpha for Lasso regression using cross-validation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:01:06.661054Z",
     "start_time": "2026-02-21T18:01:05.185733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# lasso pipeline\n",
    "lasso_cv_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(alphas=alphas, cv=5))\n",
    "])\n",
    "\n",
    "lasso_cv_pipe.fit(X_train, y_train)\n",
    "best_lasso_alpha = lasso_cv_pipe.named_steps['regressor'].alpha_\n",
    "print(f\"Optimized Lasso Alpha: {best_lasso_alpha}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Lasso Alpha: 0.001\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The best alpha for Lasso regression is `0.001` this value is the one that minimizes the mean squared error during cross-validation."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T18:02:47.998148Z",
     "start_time": "2026-02-21T18:02:45.008823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# RidgeCV pipeline\n",
    "ridge_cv_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RidgeCV(alphas=alphas, cv=5))\n",
    "])\n",
    "\n",
    "# 3. Fit the model to find the best alpha\n",
    "ridge_cv_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 4. Retrieve the best alpha\n",
    "best_ridge_alpha = ridge_cv_pipe.named_steps['regressor'].alpha_\n",
    "print(f\"Optimized Ridge Alpha: {best_ridge_alpha}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Ridge Alpha: 1.0\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** The best alpha for Ridge regression is `1.0` this value is the one that minimizes the mean squared error during cross-validation."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create models\n",
    "\n",
    "I will create three models Linear Regression, Lasso Regression, and Ridge Regression, then I will calculate the mean squared error and r2_score for each model. The mse and r2_score will be calculated using the value without log transformation, so the price log transformation will be reverted using `exp1m`. The results will be stored in a dataframe for easy comparison."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T21:44:08.390783Z",
     "start_time": "2026-02-21T21:43:37.293589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loop through different models and parameters\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso Regression': Lasso(alpha=best_lasso_alpha),\n",
    "    'Ridge Regression': Ridge(alpha=best_ridge_alpha),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create the pipeline for the current model\n",
    "    current_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the model\n",
    "    current_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = current_pipeline.predict(X_test)\n",
    "\n",
    "    # revert price log transformation\n",
    "    y_pred_original = np.expm1(y_pred)\n",
    "    y_test_original = np.expm1(y_test)\n",
    "\n",
    "    # Calculate metrics (using original price)\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "    # append current results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MSE': round(mse, 4),\n",
    "        'MAE': round(mae,4),\n",
    "        'RMSE': round(np.sqrt(mse),4),\n",
    "        'R2 Score': round(r2, 4)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Model           MSE        MAE       RMSE  R2 Score\n",
       "0  Linear Regression  5.719685e+07  5013.5570  7562.8596    0.6541\n",
       "1   Lasso Regression  5.671894e+07  5009.8134  7531.1979    0.6570\n",
       "2   Ridge Regression  5.719554e+07  5013.5284  7562.7735    0.6541\n",
       "3      Random Forest  3.022684e+07  3203.1154  5497.8943    0.8172"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>5.719685e+07</td>\n",
       "      <td>5013.5570</td>\n",
       "      <td>7562.8596</td>\n",
       "      <td>0.6541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>5.671894e+07</td>\n",
       "      <td>5009.8134</td>\n",
       "      <td>7531.1979</td>\n",
       "      <td>0.6570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>5.719554e+07</td>\n",
       "      <td>5013.5284</td>\n",
       "      <td>7562.7735</td>\n",
       "      <td>0.6541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>3.022684e+07</td>\n",
       "      <td>3203.1154</td>\n",
       "      <td>5497.8943</td>\n",
       "      <td>0.8172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****Interpretation:**** From the above results, I can see Random Forest is the best performing model of the four tested with the lowest RMSE of `$5,493` and highest R2 score. This means that on average the model price prediction deviates from the actual car prices by about `$5,493`. The r2 score means the variance in car price is `81.7%`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For car model try to encode frequency encoding instead of one hot encoding\n",
    "# mode mean or median of the price to type the model with the price\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
